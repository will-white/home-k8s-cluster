---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
spec:
  chart:
    spec:
      chart: kube-prometheus-stack
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
      version: 79.12.0
  dependsOn:
    - name: rook-ceph-cluster
      namespace: rook-ceph
  install:
    remediation:
      retries: 3
  interval: 30m
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
      strategy: rollback
  values:
    additionalPrometheusRulesMap:
      recording-rules:
        groups:
          - name: recording_rules
            interval: 30s
            rules:
              # CPU usage aggregations
              - record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
                expr: kube_pod_container_resource_requests{resource="cpu"} * on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
              # Memory usage aggregations
              - record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
                expr: kube_pod_container_resource_requests{resource="memory"} * on (namespace, pod) group_left() max by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
              # Node CPU utilization
              - record: instance:node_cpu_utilisation:rate5m
                expr: 1 - avg without (cpu) (sum without (mode) (rate(node_cpu_seconds_total{mode=~"idle|iowait|steal"}[5m])))
              # Node memory utilization
              - record: instance:node_memory_utilisation:ratio
                expr: 1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
              # PVC usage percentage
              - record: kubelet_volume_stats:used_percentage
                expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100
      dockerhub-rules:
        groups:
          - name: dockerhub
            rules:
              - alert: DockerhubRateLimitRisk
                annotations:
                  summary: Kubernetes cluster Dockerhub rate limit risk
                expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
                labels:
                  severity: critical
      oom-rules:
        groups:
          - name: oom
            rules:
              - alert: OomKilled
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical
      zfs-rules:
        groups:
          - name: zfs
            rules:
              - alert: ZfsUnexpectedPoolState
                annotations:
                  summary: ZFS pool {{$labels.zpool}} on {{$labels.instance}} is in a unexpected state {{$labels.state}}
                expr: node_zfs_zpool_state{state!="online"} > 0
                labels:
                  severity: critical
      storage-rules:
        groups:
          - name: storage
            rules:
              - alert: PvcAlmostFull
                annotations:
                  summary: PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is {{ $value | humanizePercentage }} full
                  description: The persistent volume claim {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is over 85% full. Consider expanding the volume or cleaning up data.
                expr: kubelet_volume_stats:used_percentage > 85
                for: 5m
                labels:
                  severity: warning
              - alert: PvcCriticallyFull
                annotations:
                  summary: PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is {{ $value | humanizePercentage }} full
                  description: The persistent volume claim {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is over 95% full. Immediate action required.
                expr: kubelet_volume_stats:used_percentage > 95
                for: 2m
                labels:
                  severity: critical
              - alert: PvcFillingUp
                annotations:
                  summary: PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is filling up rapidly
                  description: Based on recent trends, the persistent volume claim {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is expected to fill up within 4 hours.
                expr: |
                  (
                    kubelet_volume_stats:used_percentage > 80
                  and
                    predict_linear(kubelet_volume_stats_used_bytes[6h], 4 * 3600) > kubelet_volume_stats_capacity_bytes
                  )
                for: 10m
                labels:
                  severity: warning
    alertmanager:
      alertmanagerSpec:
        externalUrl: https://alertmanager.${SECRET_DOMAIN}
        storage:
          volumeClaimTemplate:
            spec:
              resources:
                requests:
                  storage: 1Gi
              storageClassName: ceph-block
      config:
        global:
          resolve_timeout: 5m
        route:
          group_by: ['alertname', 'job', 'severity']
          group_wait: 10s
          group_interval: 10s
          repeat_interval: 12h
          receiver: 'null'
          routes:
            - matchers:
                - severity =~ "critical|warning"
              receiver: 'null'
              continue: true
            - matchers:
                - alertname = "Watchdog"
              receiver: 'null'
        receivers:
          - name: 'null'
        inhibit_rules:
          - source_matchers:
              - severity = "critical"
            target_matchers:
              - severity = "warning"
            equal: ['alertname', 'namespace']
      ingress:
        annotations:
          gethomepage.dev/app: alertmanager
          gethomepage.dev/description: Alerting service
          gethomepage.dev/enabled: 'true'
          gethomepage.dev/group: Cluster & Observability
          gethomepage.dev/icon: alertmanager.png
          gethomepage.dev/name: Alertmanager
        enabled: true
        hosts: ["alertmanager.${SECRET_DOMAIN}"]
        ingressClassName: internal
        pathType: Prefix
    cleanPrometheusOperatorObjectNames: true
    crds:
      enabled: true
      upgradeJob:
        enabled: true
        forceConflicts: true
    grafana:
      enabled: false
      forceDeployDashboards: true
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
      metricLabelsAllowlist:
        - pods=[*]
        - deployments=[*]
        - persistentvolumeclaims=[*]
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node
    kubeApiServer:
      serviceMonitor:
        selector:
          k8s-app: kube-apiserver
    kubeControllerManager:
      service: &ref_0
        selector:
          k8s-app: kube-controller-manager
    kubeEtcd:
      service: *ref_0
    kubeProxy:
      enabled: false
    kubeScheduler:
      service:
        selector:
          k8s-app: kube-scheduler
    prometheus:
      ingress:
        annotations:
          gethomepage.dev/app: prometheus
          gethomepage.dev/description: Monitoring Scrape Service
          gethomepage.dev/enabled: 'true'
          gethomepage.dev/group: Cluster & Observability
          gethomepage.dev/icon: prometheus.png
          gethomepage.dev/name: Prometheus
          gethomepage.dev/widget.type: prometheus
          gethomepage.dev/widget.url: http://kube-prometheus-stack-prometheus.observability:9090
        enabled: true
        hosts: ["prometheus.${SECRET_DOMAIN}"]
        ingressClassName: internal
        pathType: Prefix
      prometheusSpec:
        enableAdminAPI: true
        enableFeatures:
          - memory-snapshot-on-shutdown
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        query:
          timeout: 2m
          maxConcurrency: 20
          maxSamples: 50000000
        resources:
          requests:
            cpu: 100m
        retention: 14d
        retentionSize: 100GB
        ruleSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        storageSpec:
          volumeClaimTemplate:
            spec:
              resources:
                requests:
                  storage: 100Gi
              storageClassName: ceph-block
        walCompression: true
    prometheus-node-exporter:
      fullnameOverride: node-exporter
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node
